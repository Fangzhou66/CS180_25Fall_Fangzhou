<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 3 · Auto-Stitching Photo Mosaics</title>
  <meta name="description" content="CS180 Project 3 report — Auto-stitching photo mosaics with homographies and blending." />
  <meta name="robots" content="noindex" />
  <style>
    :root {
      --fg: #0b1120;
      --muted: #475569;
      --accent: #2563eb;
      --bg: #ffffff;
      --panel: #f8fafc;
      --border: #e2e8f0;
      --radius: 14px;
      --shadow: 0 12px 36px rgba(15, 23, 42, 0.08);
      --max-width: 1024px;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      color: var(--fg);
      background: var(--bg);
      line-height: 1.65;
      -webkit-font-smoothing: antialiased;
    }
    header {
      background: linear-gradient(160deg, #e0e7ff 0%, #ffffff 60%);
      border-bottom: 1px solid var(--border);
      padding: 48px 20px 56px;
    }
    .container {
      width: 100%;
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 0 20px;
    }
    h1 {
      font-size: clamp(32px, 5vw, 44px);
      margin: 0 0 12px;
      letter-spacing: -0.02em;
    }
    h2 {
      font-size: clamp(24px, 4vw, 30px);
      margin: 48px 0 12px;
      letter-spacing: -0.01em;
    }
    h3 {
      font-size: 18px;
      margin: 20px 0 8px;
      color: #111827;
    }
    p {
      color: var(--muted);
      margin: 12px 0;
    }
    a {
      color: var(--accent);
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .lead {
      max-width: 680px;
      font-size: 18px;
      color: #1f2937;
      margin-top: 12px;
    }
    .panel {
      background: var(--panel);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 24px;
      margin: 28px 0;
      box-shadow: var(--shadow);
    }
    .meta-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
      gap: 12px;
      margin-top: 18px;
    }
    .meta-card {
      padding: 14px 16px;
      border-radius: 12px;
      background: rgba(37, 99, 235, 0.08);
      color: #1d4ed8;
      font-weight: 500;
    }
    .toc {
      margin-top: 32px;
      display: grid;
      gap: 8px;
      max-width: 420px;
    }
    .toc a {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 16px;
      border-radius: 10px;
      border: 1px solid var(--border);
      background: #ffffff;
      color: var(--fg);
      font-weight: 500;
      transition: transform 120ms ease, box-shadow 120ms ease;
    }
    .toc a span {
      color: var(--muted);
      font-size: 14px;
      font-weight: 400;
    }
    .toc a:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 24px rgba(15, 23, 42, 0.1);
    }
    main {
      padding: 48px 0 80px;
    }
    section + section {
      margin-top: 28px;
    }
    .gallery {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 18px;
      margin: 24px 0;
    }
    figure {
      margin: 0;
      background: #ffffff;
      border: 1px solid var(--border);
      border-radius: var(--radius);
      overflow: hidden;
      box-shadow: 0 20px 40px rgba(15, 23, 42, 0.08);
    }
    figure img {
      width: 100%;
      display: block;
    }
    figcaption {
      padding: 14px 16px 16px;
      font-size: 14px;
      color: var(--muted);
    }
    .side-by-side {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 18px;
      margin: 24px 0;
    }
    .callout {
      border-left: 4px solid var(--accent);
      padding: 16px 22px;
      background: rgba(37, 99, 235, 0.05);
      border-radius: 12px;
      color: #1e3a8a;
      font-size: 15px;
    }
    blockquote {
      margin: 24px 0;
      padding: 0 0 0 18px;
      border-left: 4px solid var(--accent);
      color: #1f2937;
      font-style: italic;
      background: rgba(37, 99, 235, 0.05);
    }
    ul {
      margin: 16px 0;
      padding-left: 20px;
      color: var(--muted);
    }
    footer {
      border-top: 1px solid var(--border);
      padding: 28px 20px 36px;
      background: #f8fafc;
      color: var(--muted);
      text-align: center;
      font-size: 14px;
    }
    @media (max-width: 640px) {
      header { padding: 40px 20px 48px; }
      .panel { padding: 20px; }
    }
  </style>
</head>
<body>
  <header>
    <div class="container">
      <h1>Project 3: Auto-Stitching Photo Mosaics</h1>
      <p class="lead">
        Manually selected correspondences, homography estimation, inverse warping, and multi-band blending come together to align everyday scenes into seamless panoramas.
      </p>
      <div class="meta-grid">
        <div class="meta-card">CS180 · Fall 2025</div>
        <div class="meta-card">Author · Fangzhou</div>
        <div class="meta-card">Tools · NumPy · OpenCV · Matplotlib</div>
      </div>
      <nav class="toc">
        <a href="#overview">Project overview <span>high-level goals &amp; challenges</span></a>
        <a href="#part-a1">Part A.1 · Capture <span>careful image acquisition</span></a>
        <a href="#part-a2">Part A.2 · Homographies <span>solving for planar projections</span></a>
        <a href="#part-a3">Part A.3 · Warping <span>nearest vs. bilinear interpolation</span></a>
        <a href="#part-a4">Part A.4 · Mosaics <span>masking, blending, takeaways</span></a>
        <a href="#part-b1">Part B.1 · Corners <span>Harris + ANMS detection</span></a>
        <a href="#part-b2">Part B.2 · Descriptors <span>compact patch embeddings</span></a>
        <a href="#part-b3">Part B.3 · Matching <span>ratio test correspondences</span></a>
        <a href="#part-b4">Part B.4 · RANSAC <span>robust homography estimation</span></a>
      </nav>
    </div>
  </header>
  <main>
    <div class="container">
      <section id="overview">
        <h2>Project Overview</h2>
        <p>
          The goal of this project is to automatically align and stitch overlapping photographs into wide mosaics.
          I selected multiple scenes, gathered shots by rotating around a fixed center of projection, and then
          built a pipeline that (1) gathers manual point correspondences, (2) solves for planar homographies with
          least squares, (3) performs inverse mapping with either nearest-neighbor or bilinear interpolation, and
          (4) blends the warped views into a single composite using soft masks.
        </p>
        <p>
          The implementation leans on the Direct Linear Transform (DLT) formulation for homography estimation,
          extends the images with padding to give the warp room to breathe, and blends contributions by averaging
          each pixel according to per-image weight masks. Below I document each stage with representative visuals
          and observations.
        </p>
        <div class="panel">
          <h3>Key lessons while building the pipeline</h3>
          <ul>
            <li>Keeping the camera stationary except for rotation is crucial—small translations manifest as parallax and break planar assumptions.</li>
            <li>Numerical stability matters: normalizing coordinates before solving the DLT system improves homography estimates when points cluster.</li>
            <li>Inverse warping avoids holes, but interpolation quality dictates visual fidelity; bilinear interpolation is worth the extra compute.</li>
            <li>Alpha-style masks that taper near the overlap prevent ghosts far better than hard cuts between warped images.</li>
          </ul>
        </div>
      </section>

      <section id="part-a1">
        <h2>Part A.1 · Shoot the Pictures</h2>
        <p>
          Each sequence below was captured handheld by pivoting the phone around the same point, ensuring the center
          of projection stays fixed. I bracketed overlap to guarantee robust correspondence picking later on.
        </p>

        <h3>Outside Evans Hall · three-shot sweep</h3>
        <div class="gallery">
          <figure>
            <img src="IMG_0123.jpg" alt="Outside Evans Hall — left frame" loading="lazy" />
            <figcaption>Left exposure highlights the Doe Library windows flanking Evans Hall.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0124.jpg" alt="Outside Evans Hall — middle frame" loading="lazy" />
            <figcaption>Middle exposure centers the rows of parked bicycles along the plaza.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0125.jpg" alt="Outside Evans Hall — right frame" loading="lazy" />
            <figcaption>Right exposure pulls in a single bicycle with the new engineering hall in the background.</figcaption>
          </figure>
        </div>

        <h3>Amazon Hub Locker · quick panorama</h3>
        <div class="gallery">
          <figure>
            <img src="IMG_0117.jpg" alt="Amazon Hub Locker — left frame" loading="lazy" />
            <figcaption>Left exposure frames the connection toward the Martin Luther King Jr. Student Union.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0118.jpg" alt="Amazon Hub Locker — middle frame" loading="lazy" />
            <figcaption>Middle exposure spotlights locker columns 1–4 with their bright yellow doors.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0119-2.jpg" alt="Amazon Hub Locker — right frame" loading="lazy" />
            <figcaption>Right exposure continues through locker columns 3–6 and the nearby walkway.</figcaption>
          </figure>
        </div>

        <h3>Outside Materials Science &amp; Engineering · exterior sweep</h3>
        <div class="gallery">
          <figure>
            <img src="IMG_0133.jpg" alt="Materials Science exterior — left frame" loading="lazy" />
            <figcaption>Left exposure introduces the Materials Science &amp; Engineering courtyard and shaded seating.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0132.jpg" alt="Materials Science exterior — middle frame" loading="lazy" />
            <figcaption>Middle exposure keeps the building facade centered while highlighting the outdoor study tables.</figcaption>
          </figure>
          <figure>
            <img src="IMG_0131.jpg" alt="Materials Science exterior — right frame" loading="lazy" />
            <figcaption>Right exposure wraps up with the engineering complex connection and tree-lined walkway.</figcaption>
          </figure>
        </div>

        <div class="callout">
          Careful capture makes the downstream math forgiving: strong overlap and minimal camera translation
          produced clean correspondences, even across repetitive textures like lockers or glass facades.
        </div>
      </section>

      <section id="part-a2">
        <h2>Part A.2 · Recover Homographies</h2>
        <p>
          Given a set of manually clicked correspondences, we solve for the 3×3 homography matrix that maps points
          in one image to another. Because the homography is scale-invariant, it contains eight degrees of freedom,
          so we need at least four non-collinear pairs. I normalize point coordinates, stack the equations from each
          correspondence into a design matrix, and then solve the resulting least-squares problem with SVD.
        </p>

        <div class="panel">
          <strong>DLT formulation</strong>
          <p>
            For each correspondence (x, y) -> (x', y') we append two rows to the system A * h = 0:
          </p>
          <pre><code>
[ -x  -y  -1   0   0   0   x'*x   x'*y   x' ]
[  0   0   0  -x  -y  -1   y'*x   y'*y   y' ]
          </code></pre>
          <p>
            Setting h33 = 1 removes the scale ambiguity. Solving the overdetermined system with SVD yields the
            homography that best aligns the point pairs in the least-squares sense.
          </p>
        </div>

        <div class="side-by-side">
          <figure>
            <img src="part_a2/desk_1_correspondence.jpg" alt="Desk scene correspondences for the first frame" loading="lazy" />
            <figcaption>Desk (left): points emphasize corners on the monitor, keyboard, and desk edges.</figcaption>
          </figure>
          <figure>
            <img src="part_a2/desk_2_correspondence.jpg" alt="Desk scene correspondences for the second frame" loading="lazy" />
            <figcaption>Desk (right): matching features across the two viewpoints define the homography.</figcaption>
          </figure>
        </div>

        <div class="panel">
          <strong>Recovered homography</strong>
          <p>
            The left-to-right desk alignment produces the following homography (scaled so h33 = 1):
          </p>
          <pre><code>
[ 1.98737628  -0.04391823  -4121.45364 ]
[ 0.29124797   1.63357928  -1138.49450 ]
[ 0.00015967   0.00000286     1.00000000 ]
          </code></pre>
          <p>
            The off-diagonal entries capture the planar rotation/shear between the two photos, while the translation
            column accounts for the shift created as the camera pivots around the desk setup.
          </p>
        </div>
      </section>

      <section id="part-a3">
        <h2>Part A.3 · Warp the Images</h2>
        <p>
          With homographies in hand, I implemented two inverse-warping strategies. Nearest-neighbor simply samples
          the closest pixel in the source image. Bilinear interpolation, on the other hand, interpolates the four
          surrounding pixels and smooths the result. Both methods iterate over pixels in the destination frame and
          map them back into the source via H<sup>−1</sup>.
        </p>

        <div class="side-by-side">
          <figure>
            <img src="IMG_0138.jpg" alt="Original input photograph" loading="lazy" />
          </figure>
          <figure>
            <img src="part_a3/channing_court_nnwarp.jpg" alt="Nearest neighbor warp result" loading="lazy" />
          </figure>
          <figure>
            <img src="part_a3/channing_court_blwarp.jpg" alt="Bilinear warp result" loading="lazy" />
          </figure>
        </div>

        <div class="side-by-side">
          <figure>
            <img src="IMG_0145.jpg" alt="Original input photograph" loading="lazy" />
          </figure>
          <figure>
            <img src="part_a3/roomates_nnwarp.jpg" alt="Nearest neighbor warp result" loading="lazy" />
          </figure>
          <figure>
            <img src="part_a3/roomates_blwarp.jpg" alt="Bilinear warp result" loading="lazy" />
          </figure>
        </div>

        <p>
          In both cases the bilinear variant noticeably reduces ringing and aliasing. For the final mosaics I default
          to bilinear interpolation—it balances crispness with smooth gradients, especially in low-texture regions
          such as skies and whiteboards.
        </p>
      </section>

      <section id="part-a4">
        <h2>Part A.4 · Blend the Images into a Mosaic</h2>
        <p>
          After padding each frame with generous black borders, I warp the neighbors into the coordinate system of the
          anchor image. Each warped image receives a soft mask that ramps from 1 in the center to 0 near the edges.
          Summing the RGB values weighted by these masks and normalizing by the accumulated weights yields seamless
          composites without abrupt transitions.
        </p>

        <div class="gallery">
          <figure>
            <img src="part_a4/during_lecture.jpg" alt="Panorama composed outside Evans Hall" loading="lazy" />
            <figcaption>Outside Evans Hall: the stitched panorama spans Doe Library, the bike racks, and the new engineering hall.</figcaption>
          </figure>
          <figure>
            <img src="part_a4/outside_soda.jpg" alt="Panorama composed across the Amazon Hub Locker" loading="lazy" />
            <figcaption>Amazon Hub Locker: lockers 1–6 and the adjacent student union corridor flow together without visible seams.</figcaption>
          </figure>
          <figure>
            <img src="part_a4/inside_sky.jpg" alt="Panorama composed outside Materials Science and Engineering" loading="lazy" />
            <figcaption>Materials Science &amp; Engineering: courtyard seating, facade, and tree-lined walkway blend into a single wide shot.</figcaption>
          </figure>
        </div>

        <p>
          Ghosting is minimal thanks to steady capture, yet faint seams can appear when moving subjects (people in
          class, swaying leaves) drift between exposures. A future improvement would be to implement multi-band
          blending to better hide these residual differences.
        </p>

      </section>

      <section id="part-b1">
        <h2>Part B.1 · Harris Corner Detection</h2>
        <p>
          I start by running a Harris detector implemented with <code>skimage.feature.corner_harris</code> (sigma = 1),
          which produces a dense response map over the grayscale image. Peaks are harvested by <code>peak_local_max</code>
          with a 10-pixel exclusion radius, and I discard anything within 20 pixels of the border so later warps do not
          rely on unstable edge measurements. The result is a few thousand well-spaced candidates whose score reflects
          how corner-like each location is.
        </p>
        <p>
          To avoid cramming all features around the brightest monitors, I apply Adaptive Non-Maximal Suppression (ANMS).
          Candidates are sorted by their Harris response and, for each point, I record the minimum Euclidean distance to
          any stronger point whose response is at least 90% as large. This distance becomes the suppression radius; taking
          the 500 points with the largest radii yields a set of corners that blanket the scene evenly, which is crucial
          for the descriptor and matching stages that follow.
        </p>
        <div class="panel">
          <strong>ANMS implementation</strong>
          <ol>
            <li>Compute the Harris response map with <code>corner_harris(..., sigma=1)</code>, pick local maxima using <code>peak_local_max</code>, and remove detections within 20 pixels of the image border.</li>
            <li>Sort the surviving candidates by their Harris strength from strongest to weakest before evaluating ANMS.</li>
            <li>For each candidate, measure the smallest distance to a previously processed stronger corner whose response is at least 0.9× the current response; store this value as the suppression radius.</li>
            <li>Rank candidates by suppression radius and keep the top <em>k</em> = 500 so the remaining corners stay spatially diffuse.</li>
          </ol>
        </div>
        <div class="gallery">
          <figure>
            <img src="part_b1/lecture_1_harris_corners.png" alt="Inside Sky Lab Harris corner heatmap" loading="lazy" />
            <figcaption>Inside Sky Lab · raw Harris corner responses before suppression.</figcaption>
          </figure>
          <figure>
            <img src="part_b1/lecture_1_strongest_500_harris_corners.png" alt="Inside Sky Lab strongest Harris corners" loading="lazy" />
            <figcaption>Inside Sky Lab · naively picking the 500 strongest responses clusters around the monitors.</figcaption>
          </figure>
          <figure>
            <img src="part_b1/output3.png" alt="Inside Sky Lab ANMS filtered corners" loading="lazy" />
            <figcaption>Inside Sky Lab · ANMS spreads 500 corners evenly across the lab interior.</figcaption>
          </figure>
        </div>
      </section>

      <section id="part-b2">
        <h2>Part B.2 · Feature Descriptor Extraction</h2>
        <p>
          This stage implements the feature descriptor extraction. For every detected interest point, I crop a 40 × 40
          window in the original image, blur it slightly, and downsample to an 8 × 8 patch that becomes the descriptor.
          The compact representation is easy to compare while still encoding the local intensity structure.
        </p>
        <p>
          All descriptors remain axis-aligned because the Inside Sky Lab captures do not involve any significant
          in-plane rotation. Below are three sample descriptors pulled from the Inside Sky Lab left image—each retains
          enough contrast to make matching reliable even after normalization.
        </p>
        <div class="gallery">
          <figure>
            <img src="part_b2/lecture_1_descriptor_patch_0.png" alt="Inside Sky Lab descriptor patch example 1" loading="lazy" />
            <figcaption>Inside Sky Lab descriptor example · computer screen.</figcaption>
          </figure>
          <figure>
            <img src="part_b2/lecture_1_descriptor_patch_1.png" alt="Inside Sky Lab descriptor patch example 2" loading="lazy" />
            <figcaption>Inside Sky Lab descriptor example · smooth gradient across the lab wall and workspace.</figcaption>
          </figure>
          <figure>
            <img src="part_b2/lecture_1_descriptor_patch_2.png" alt="Inside Sky Lab descriptor patch example 3" loading="lazy" />
            <figcaption>Inside Sky Lab descriptor example · high-frequency structure from the equipment shelving.</figcaption>
          </figure>
        </div>
      </section>

      <section id="part-b3">
        <h2>Part B.3 · Feature Matching</h2>
        <p>
          The matching stage sweeps through every descriptor in the Inside Sky Lab reference image, measures SSD error
          against descriptors in the second view, and tracks both the best and second-best candidates. Retaining a match
          only when the ratio of the two errors is below 0.7 enforces Lowe's ratio test and filters out ambiguous
          correspondences along repeated textures.
        </p>
        <figure>
          <img src="part_b3/lecture_1_2_matches.png" alt="Inside Sky Lab descriptor matches after the 0.7 ratio test" loading="lazy" />
          <figcaption>Inside Sky Lab matches · red lines show the inliers that pass the ratio test and survive to RANSAC.</figcaption>
        </figure>
      </section>

      <section id="part-b4">
        <h2>Part B.4 · RANSAC for Robust Homography</h2>
        <p>
          RANSAC repeatedly samples minimal sets of correspondences, estimates candidate homographies, and measures
          agreement across all matches. By focusing on the hypothesis with the highest inlier count, the algorithm
          resists outliers from mismatched descriptors and produces a stable warp for the panorama.
        </p>
        <div class="panel">
          <strong>RANSAC loop</strong>
          <ol>
            <li>Randomly sample four matches from the ratio-test survivors.</li>
            <li>Estimate a homography via DLT and normalize so h<sub>33</sub> = 1.</li>
            <li>Project all points, compute their reprojection errors, and count inliers.</li>
            <li>Repeat for a fixed number of iterations to cover multiple hypotheses.</li>
            <li>Re-fit the homography using every inlier from the best hypothesis and return it.</li>
          </ol>
        </div>
        <p>
          The auto-stitched mosaics below compare the manual correspondences from Part A with the fully automatic
          pipeline. RANSAC removes spurious matches and keeps the alignment sharp across lab interiors and outdoor
          scenes alike.
        </p>
        <div class="gallery">
          <figure>
            <img src="part_a4/outside_soda.jpg" alt="Amazon Hub Locker manual mosaic" loading="lazy" />
            <figcaption>Amazon Hub Locker (Stitched Manually) · lockers 1–6 and the adjacent student union corridor flow together without visible seams.</figcaption>
          </figure>
          <figure>
            <img src="part_b4/during_lecture_auto.jpg" alt="Amazon Hub Locker panorama stitched with RANSAC" loading="lazy" />
            <figcaption>Amazon Hub Locker in Student Center (Stitched with RANSAC)</figcaption>
          </figure>
          <figure>
            <img src="part_b4/outside_sky_auto.jpg" alt="Outside Soda panorama stitched with RANSAC" loading="lazy" />
            <figcaption>Outside Soda (Stitched with RANSAC)</figcaption>
          </figure>
          <figure>
            <img src="../output4.png" alt="Outside Soda manual mosaic" loading="lazy" />
            <figcaption>Outside Soda (Stitched Manually) · locker corridor and lounge signage align without ghosting.</figcaption>
          </figure>
          <figure>
            <img src="part_b4/inside_sky_auto.jpg" alt="Inside Sky Lab atrium stitched with RANSAC" loading="lazy" />
            <figcaption>Inside Sky Lab Atrium (Stitched with RANSAC)</figcaption>
          </figure>
          <figure>
            <img src="../output6.png" alt="Inside Sky Lab atrium manual mosaic" loading="lazy" />
            <figcaption>Inside Sky Lab Atrium (Stitched Manually) · whiteboard, desks, and ceiling beams remain coherent across the sweep.</figcaption>
          </figure>
        </div>
      </section>

    </div>
  </main>

  <footer>
    © Fangzhou · CS180 — Project 3 Auto-Stitching Photo Mosaics
  </footer>
</body>
</html>
