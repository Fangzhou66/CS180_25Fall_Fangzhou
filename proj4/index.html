<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 4 · Neural Radiance Fields</title>
  <meta name="description" content="CS180 Project 4 report — Neural radiance fields for 2D image fitting and multi-view 3D reconstruction." />
  <meta name="robots" content="noindex" />
  <style>
    :root {
      --fg: #0b1120;
      --muted: #475569;
      --accent: #2563eb;
      --bg: #ffffff;
      --panel: #f8fafc;
      --border: #e2e8f0;
      --radius: 14px;
      --shadow: 0 12px 36px rgba(15, 23, 42, 0.08);
      --max-width: 1024px;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      color: var(--fg);
      background: var(--bg);
      line-height: 1.65;
      -webkit-font-smoothing: antialiased;
    }
    header {
      background: linear-gradient(160deg, #e0e7ff 0%, #ffffff 60%);
      border-bottom: 1px solid var(--border);
      padding: 48px 20px 56px;
    }
    .container {
      width: 100%;
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 0 20px;
    }
    h1 {
      font-size: clamp(32px, 5vw, 44px);
      margin: 0 0 12px;
      letter-spacing: -0.02em;
    }
    h2 {
      font-size: clamp(24px, 4vw, 30px);
      margin: 48px 0 12px;
      letter-spacing: -0.01em;
    }
    h3 {
      font-size: 18px;
      margin: 20px 0 8px;
      color: #111827;
    }
    p {
      color: var(--muted);
      margin: 12px 0;
    }
    a {
      color: var(--accent);
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .lead {
      max-width: 680px;
      font-size: 18px;
      color: #1f2937;
      margin-top: 12px;
    }
    .panel {
      background: var(--panel);
      border: 1px solid var(--border);
      border-radius: var(--radius);
      padding: 24px;
      margin: 28px 0;
      box-shadow: var(--shadow);
    }
    .meta-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
      gap: 12px;
      margin-top: 18px;
    }
    .meta-card {
      padding: 14px 16px;
      border-radius: 12px;
      background: rgba(37, 99, 235, 0.08);
      color: #1d4ed8;
      font-weight: 500;
    }
    .toc {
      margin-top: 32px;
      display: grid;
      gap: 8px;
      max-width: 420px;
    }
    .toc a {
      display: inline-flex;
      align-items: center;
      gap: 10px;
      padding: 12px 16px;
      border-radius: 10px;
      border: 1px solid var(--border);
      background: #ffffff;
      color: var(--fg);
      font-weight: 500;
      transition: transform 120ms ease, box-shadow 120ms ease;
    }
    .toc a span {
      color: var(--muted);
      font-size: 14px;
      font-weight: 400;
    }
    .toc a:hover {
      transform: translateY(-2px);
      box-shadow: 0 10px 24px rgba(15, 23, 42, 0.1);
    }
    main {
      padding: 48px 0 80px;
    }
    section + section {
      margin-top: 28px;
    }
    .gallery {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 20px;
      margin: 20px 0 8px;
    }
    .gallery-two {
      grid-template-columns: repeat(auto-fit, minmax(360px, 1fr));
    }
    figure {
      margin: 0;
      background: #ffffff;
      border: 1px solid var(--border);
      border-radius: var(--radius);
      overflow: hidden;
      box-shadow: 0 20px 40px rgba(15, 23, 42, 0.08);
    }
    figure img {
      width: 100%;
      display: block;
    }
    figcaption {
      padding: 14px 16px 16px;
      font-size: 14px;
      color: var(--muted);
    }
    .gallery .wide {
      grid-column: 1 / -1;
    }
    .placeholder {
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 40px 16px;
      background: repeating-linear-gradient(
        45deg,
        #e5e7eb,
        #e5e7eb 10px,
        #f9fafb 10px,
        #f9fafb 20px
      );
      color: #4b5563;
      font-size: 14px;
      text-align: center;
    }
    .side-by-side {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 18px;
      margin: 24px 0;
    }
    .callout {
      border-left: 4px solid var(--accent);
      padding: 16px 22px;
      background: rgba(37, 99, 235, 0.05);
      border-radius: 12px;
      color: #1e3a8a;
      font-size: 15px;
    }
    ul {
      margin: 16px 0;
      padding-left: 20px;
      color: var(--muted);
    }
    footer {
      border-top: 1px solid var(--border);
      padding: 28px 20px 36px;
      background: #f8fafc;
      color: var(--muted);
      text-align: center;
      font-size: 14px;
    }
    @media (max-width: 640px) {
      header { padding: 40px 20px 48px; }
      .panel { padding: 20px; }
    }
  </style>
</head>
<body>
  <header>
    <div class="container">
      <h1>Project 4: Neural Radiance Fields</h1>
      <p class="lead">
        In this project I move from classic image processing to neural fields, first fitting a 2D neural field to a single image and then training a full NeRF model for multi-view 3D reconstruction and novel view synthesis.
      </p>
      <div class="meta-grid">
        <div class="meta-card">CS180 · Fall 2025</div>
        <div class="meta-card">Author · Fangzhou</div>
        <div class="meta-card">Tools · PyTorch · NumPy · Viser</div>
      </div>
      <nav class="toc">
        <a href="#part0"><span>Part 0</span> Camera calibration &amp; capture</a>
        <a href="#part1"><span>Part 1</span> 2D neural field fitting</a>
        <a href="#part2"><span>Part 2</span> NeRF from multi-view images</a>
        <a href="#part26"><span>Part 2.6</span> Training on my own data</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="container">
      <section id="part0">
        <h2>Part 0: Calibrating Your Camera and Capturing a 3D Scan</h2>
        <p>
          In Part 0 I set up the data acquisition pipeline that the rest of the project depends on. I estimate the intrinsic parameters of my phone camera, take a sequence of photos of the object while moving around it, and correct lens distortion so that the images better match an ideal pinhole camera model.
        </p>
        <p>
          The figures below include a Viser visualization of the recovered camera poses as well as side‑by‑side comparisons of raw distorted frames and their undistorted versions, highlighting how calibration changes the geometry of the images.
        </p>

        <div class="gallery gallery-two">
          <figure>
            <img src="Screenshot 2025-11-15 at 21.37.13.png" alt="Image 1 from the captured dataset" loading="lazy" />
            <figcaption>Image 1 from the captured dataset of the object.</figcaption>
          </figure>
          <figure>
            <img src="Screenshot 2025-11-15 at 22.42.30.png" alt="Image 2 from the captured dataset" loading="lazy" />
            <figcaption>Image 2 from the captured dataset of the object.</figcaption>
          </figure>
        </div>
        <div class="gallery">
          <figure class="wide">
            <img src="ovc.png" alt="Comparison between the original and undistorted images" loading="lazy" />
            <figcaption>Comparison between the original and undistorted images.</figcaption>
          </figure>
        </div>
      </section>

      <section id="part1">
        <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
        <p>
          As a warm‑up to full NeRFs, I train a coordinate‑based neural network that memorizes a single image. Each pixel location is treated as a continuous 2D coordinate, which I encode with sinusoidal positional features before feeding into a small MLP that outputs RGB values.
        </p>
        <p>
          The network consists of four fully connected layers with ReLU activations and a final sigmoid to keep colors in \[0, 1]. I optimize the parameters with Adam (learning rate 0.01) and sweep over both the number of positional encoding frequencies and the hidden layer width to see how model capacity and high‑frequency encoding influence reconstruction quality.
        </p>
        <div class="panel">
          <h3>Architecture and training details</h3>
          <ul>
            <li>Inputs are 2D coordinates, transformed with sinusoidal positional embeddings.</li>
            <li>The network uses 4 fully-connected layers with ReLU, followed by a sigmoid output layer.</li>
            <li>Adam optimizer with learning rate 0.01 is used to minimize mean squared error between predicted and ground-truth RGB.</li>
            <li>I vary both embedding frequencies and hidden layer width to study reconstruction fidelity and overfitting.</li>
          </ul>
        </div>

        <p>
          During training I periodically snapshot the model, render its current reconstruction, and log PSNR so I can watch the neural field gradually sharpen from a blurry guess into a high‑fidelity copy of the target image.
        </p>

        <div class="gallery">
          <figure>
            <div class="placeholder">Placeholder — Homography matrix visualization</div>
            <figcaption>Homography matrix or coordinate transform used when relating pixel coordinates to the neural field.</figcaption>
          </figure>
          <figure>
            <div class="placeholder">Placeholder — 2D image reconstruction</div>
            <figcaption>Reconstruction of the 2D image at a training checkpoint.</figcaption>
          </figure>
          <figure>
            <div class="placeholder">Placeholder — Homography matrix (alternate)</div>
            <figcaption>Alternate view of the homography matrix used in experiments.</figcaption>
          </figure>
        </div>
        <div class="gallery">
          <figure class="wide">
            <img src="psnr_curves.png" alt="Training PSNR curve for the 2D neural field" loading="lazy" />
            <figcaption>Training PSNR curve showing convergence over time.</figcaption>
          </figure>
        </div>
      </section>

      <section id="part2">
        <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
        <p>
          In Part 2 I move from fitting a single image to learning a full 3D radiance field. Using a set of posed photographs, I build the geometry and rendering machinery needed for NeRF: turning pixels into rays, sampling 3D points along those rays, querying a neural network for color and density, and integrating the result along each ray to render novel views.
        </p>

        <h3>Part 2.1 — Ray geometry: <code>transform</code>, <code>pixel_to_camera</code>, <code>pixel_to_ray</code></h3>
        <p>
          I start by implementing three core batched geometry routines: <code>transform</code>, <code>pixel_to_camera</code>, and <code>pixel_to_ray</code>. Together they connect 2D pixel coordinates to 3D rays in world space.
        </p>
        <ul>
          <li><code>transform</code> applies a homogeneous 4×4 transform to a batch of points and then drops back to 3D coordinates.</li>
          <li><code>pixel_to_camera</code> maps pixel locations into the camera coordinate frame; since a pixel corresponds to an entire ray, I introduce a scale parameter to choose a specific point along that ray.</li>
          <li><code>pixel_to_ray</code> combines intrinsics and extrinsics to produce ray origins and directions that NeRF will later sample along.</li>
        </ul>

        <h3>Part 2.2 — Sampling points along rays</h3>
        <p>
          Next I design the sampling strategy. In <code>sample_ray_from_images</code> I randomly pick pixels from the training images, convert them to rays with <code>pixel_to_ray</code>, and then place <code>num_samples</code> points along each ray between <code>near</code> and <code>far</code>. I optionally jitter the samples within their intervals to implement stratified sampling and reduce aliasing.
        </p>

        <h3>Part 2.3 — RayDataset and visualizing rays with Viser</h3>
        <p>
          I encapsulate this logic in a <code>RayDataset</code> that yields mini‑batches of rays and sample points for training. To sanity‑check the implementation and build geometric intuition, I visualize subsets of rays from the bulldozer scene in Viser.
        </p>

        <div class="gallery">
          <figure>
            <img src="Screenshot 2025-11-15 at 21.38.21.png" alt="Ray visualization from a single image" loading="lazy" />
            <figcaption>Rays emitted from one camera pose, colored by origin or direction.</figcaption>
          </figure>
          <figure>
            <img src="Screenshot 2025-11-15 at 21.37.57.png" alt="100 rays sampled from all images" loading="lazy" />
            <figcaption>100 rays sampled from all input images, showing coverage across the scene.</figcaption>
          </figure>
        </div>

        <h3>Part 2.4 — NeRF model architecture</h3>
        <p>
          The NeRF network itself takes as input 3D positions together with viewing directions, encodes them with positional embeddings, and predicts both volume density and emitted color at each sampled point.
        </p>
        <div class="panel">
          <h3>Model design</h3>
          <ul>
            <li>Separate positional encodings for 3D locations and viewing directions.</li>
            <li>A deep MLP backbone with skip connections to preserve low‑frequency information.</li>
            <li>Distinct output branches for density (σ) and RGB color.</li>
          </ul>
        </div>

        <div class="gallery">
          <figure>
            <img src="Screenshot 2025-11-15 at 22.38.35.png" alt="NeRF model architecture diagram" loading="lazy" />
            <figcaption>Diagram of the NeRF model architecture.</figcaption>
          </figure>
        </div>

        <h3>Part 2.5 — Volume rendering and bulldozer reconstruction</h3>
        <p>
          Finally, I write the differentiable volume rendering routine that turns NeRF outputs into pixels. For each ray, it accumulates contributions from all sampled points using their predicted densities and colors to produce a single RGB value.
        </p>
        <p>
          I use PyTorch’s <code>cumprod</code> to efficiently compute the transmittance terms <code>T_i</code> along each ray, avoiding explicit Python loops and making rendering fast enough for training on many rays per batch.
        </p>

        <div class="gallery">
          <figure class="wide">
            <img src="nerf_images_grid.png" alt="Training progression, validation PSNR curve, and final 3D bulldozer reconstruction" loading="lazy" />
            <figcaption>Training progression, validation PSNR curve, and final 3D bulldozer reconstruction visualized in a single grid.</figcaption>
          </figure>
          <figure>
            <img src="nerf_psnr_curve.png" alt="PSNR values evaluated on the validation set" loading="lazy" />
            <figcaption>PSNR values evaluated on the validation set.</figcaption>
          </figure>
          <figure>
            <img src="bulldozer.gif" alt="Reconstruction of the bulldozer" loading="lazy" />
            <figcaption>Reconstruction of the bulldozer.</figcaption>
          </figure>
        </div>
      </section>

      <section id="part26">
        <h2>Part 2.6: Training with My Own Data</h2>
        <p>
          To finish the project, I collect my own multi‑view dataset using an iPhone 17 Pro and reuse the same NeRF
          architecture as in Part 2.4. Rather than redesigning the network, I mainly retune the sampling range along
          each ray by adjusting the <code>near</code> and <code>far</code> bounds to match the size and distance of my scene.
        </p>
        <p>
          The resulting reconstruction is noticeably noisier than the bulldozer example. In several viewpoints the
          calibration tags end up hidden behind the object, so NeRF never observes consistent supervision from all
          directions and struggles to fully recover the geometry and appearance of those occluded regions.
        </p>

        <div class="gallery">
          <figure>
            <img src="nerf_nailong_grid.png" alt="Training progression for my NeRF data" loading="lazy" />
            <figcaption>Training progression for my personal NeRF dataset.</figcaption>
          </figure>
          <figure>
            <img src="training_loss.png" alt="Training loss for my NeRF model" loading="lazy" />
            <figcaption>Training loss curve over iterations.</figcaption>
          </figure>
          <figure>
            <img src="training_loss_psnr.png" alt="Training PSNR for my NeRF model" loading="lazy" />
            <figcaption>PSNR curve for the personal dataset.</figcaption>
          </figure>
          <figure>
            <img src="lafufu.gif" alt="Reconstruction of my own data" loading="lazy" />
            <figcaption>Final reconstruction of my own scene from novel viewpoints.</figcaption>
          </figure>
        </div>
      </section>
    </div>
  </main>

  <footer>
    <div class="container">© Fangzhou — CS180 Project 4 (for course submission/testing)</div>
  </footer>
</body>
</html>
