<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 5 · Fun with Diffusion Models</title>
  <meta name="description" content="CS180 Project 5 report — diffusion models, denoising, guidance, and flow matching." />
  <meta name="robots" content="noindex" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    :root {
      --fg: #1a1a2e;
      --muted: #64748b;
      --accent: #6366f1;
      --accent-light: #818cf8;
      --bg: #fafbfc;
      --card-bg: #ffffff;
      --panel: #f1f5f9;
      --border: #e2e8f0;
      --radius: 16px;
      --radius-sm: 10px;
      --shadow-sm: 0 2px 8px rgba(0, 0, 0, 0.04);
      --shadow: 0 4px 24px rgba(0, 0, 0, 0.06);
      --shadow-lg: 0 12px 48px rgba(0, 0, 0, 0.08);
      --max-width: 1200px;
      --transition: all 0.2s ease;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    html { scroll-behavior: smooth; }
    body {
      font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      color: var(--fg);
      background: var(--bg);
      line-height: 1.7;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }
    header {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      padding: 60px 24px 70px;
      position: relative;
      overflow: hidden;
    }
    header::before {
      content: '';
      position: absolute;
      top: 0; left: 0; right: 0; bottom: 0;
      background: url("data:image/svg+xml,%3Csvg width='60' height='60' viewBox='0 0 60 60' xmlns='http://www.w3.org/2000/svg'%3E%3Cg fill='none' fill-rule='evenodd'%3E%3Cg fill='%23ffffff' fill-opacity='0.05'%3E%3Cpath d='M36 34v-4h-2v4h-4v2h4v4h2v-4h4v-2h-4zm0-30V0h-2v4h-4v2h4v4h2V6h4V4h-4zM6 34v-4H4v4H0v2h4v4h2v-4h4v-2H6zM6 4V0H4v4H0v2h4v4h2V6h4V4H6z'/%3E%3C/g%3E%3C/g%3E%3C/svg%3E");
      opacity: 0.5;
    }
    header .container { position: relative; z-index: 1; }
    .container {
      width: 100%;
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 0 24px;
    }
    h1 {
      font-size: clamp(36px, 6vw, 52px);
      font-weight: 700;
      color: #ffffff;
      margin-bottom: 16px;
      letter-spacing: -0.03em;
      text-shadow: 0 2px 20px rgba(0,0,0,0.15);
    }
    h2 {
      font-size: clamp(22px, 3.5vw, 28px);
      font-weight: 600;
      color: var(--fg);
      margin: 56px 0 16px;
      letter-spacing: -0.02em;
      padding-bottom: 12px;
      border-bottom: 2px solid var(--accent);
      display: inline-block;
    }
    h3 {
      font-size: 17px;
      font-weight: 600;
      margin: 28px 0 12px;
      color: var(--fg);
    }
    p {
      color: var(--muted);
      margin: 14px 0;
      font-size: 15px;
      max-width: 800px;
    }
    a { color: var(--accent); text-decoration: none; transition: var(--transition); }
    a:hover { color: var(--accent-light); }
    .lead {
      font-size: 18px;
      color: rgba(255,255,255,0.9);
      margin-top: 16px;
      max-width: 700px;
      line-height: 1.8;
    }
    .meta-grid {
      display: flex;
      gap: 12px;
      margin-top: 24px;
      flex-wrap: wrap;
    }
    .meta-card {
      padding: 10px 20px;
      border-radius: 50px;
      background: rgba(255,255,255,0.2);
      backdrop-filter: blur(10px);
      color: #ffffff;
      font-weight: 500;
      font-size: 14px;
      border: 1px solid rgba(255,255,255,0.3);
    }
    .toc {
      margin-top: 36px;
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
    }
    .toc a {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 18px;
      border-radius: 50px;
      background: rgba(255,255,255,0.15);
      backdrop-filter: blur(10px);
      color: #ffffff;
      font-weight: 500;
      font-size: 13px;
      border: 1px solid rgba(255,255,255,0.25);
      transition: var(--transition);
    }
    .toc a span {
      color: rgba(255,255,255,0.7);
      font-size: 12px;
      font-weight: 400;
    }
    .toc a:hover {
      background: rgba(255,255,255,0.25);
      transform: translateY(-2px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    }
    main { padding: 60px 0 100px; }
    section {
      background: var(--card-bg);
      border-radius: var(--radius);
      padding: 40px;
      margin-bottom: 32px;
      box-shadow: var(--shadow);
      border: 1px solid var(--border);
    }
    section h2:first-child { margin-top: 0; }
    .gallery {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 24px;
      margin: 24px 0 12px;
    }
    .gallery-two { grid-template-columns: repeat(auto-fit, minmax(380px, 1fr)); }
    .gallery-wide { grid-template-columns: 1fr; }
    .gallery-row {
      display: grid;
      grid-template-columns: repeat(7, 1fr);
      gap: 16px;
      margin: 24px 0 12px;
    }
    .gallery-row figure {
      box-shadow: var(--shadow-sm);
      transition: var(--transition);
    }
    .gallery-row figure:hover {
      transform: translateY(-4px);
      box-shadow: var(--shadow);
    }
    .gallery-row figcaption {
      padding: 12px 8px;
      font-size: 11px;
      text-align: center;
      font-weight: 500;
      background: var(--panel);
    }
    .gallery-row-4 {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 20px;
      margin: 24px 0 12px;
    }
    .gallery-row-4 figure {
      box-shadow: var(--shadow-sm);
      transition: var(--transition);
    }
    .gallery-row-4 figure:hover {
      transform: translateY(-4px);
      box-shadow: var(--shadow);
    }
    .gallery-row-4 figcaption {
      padding: 12px 10px;
      font-size: 12px;
      text-align: center;
      font-weight: 500;
      background: var(--panel);
    }
    .gallery-row-6 {
      display: grid;
      grid-template-columns: repeat(6, 1fr);
      gap: 16px;
      margin: 24px 0 12px;
    }
    .gallery-row-6 figure {
      box-shadow: var(--shadow-sm);
      transition: var(--transition);
    }
    .gallery-row-6 figure:hover {
      transform: translateY(-4px);
      box-shadow: var(--shadow);
    }
    .gallery-row-6 figcaption {
      padding: 12px 8px;
      font-size: 11px;
      text-align: center;
      font-weight: 500;
      background: var(--panel);
    }
    figure {
      margin: 0;
      background: var(--card-bg);
      border: 1px solid var(--border);
      border-radius: var(--radius-sm);
      overflow: hidden;
      box-shadow: var(--shadow-sm);
      transition: var(--transition);
    }
    figure:hover {
      box-shadow: var(--shadow);
      transform: translateY(-4px);
    }
    figure img {
      width: 100%;
      display: block;
      transition: var(--transition);
    }
    figcaption {
      padding: 14px 16px;
      font-size: 13px;
      color: var(--muted);
      background: var(--panel);
      font-weight: 500;
      text-align: center;
    }
    .gallery .wide { grid-column: 1 / -1; }
    .gallery-small { justify-items: center; }
    .gallery-small figure { max-width: 600px; width: 100%; }
    ul { margin: 16px 0; padding-left: 24px; color: var(--muted); }
    .formula {
      background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
      border: 1px solid var(--border);
      border-left: 4px solid var(--accent);
      border-radius: var(--radius-sm);
      padding: 20px 24px;
      margin: 20px 0;
      overflow-x: auto;
      text-align: center;
    }
    .formula p { margin: 0; max-width: none; }
    footer {
      border-top: 1px solid var(--border);
      padding: 36px 24px 48px;
      background: var(--card-bg);
      color: var(--muted);
      text-align: center;
      font-size: 14px;
    }
    @media (max-width: 900px) {
      .gallery-row { grid-template-columns: repeat(4, 1fr); }
      .gallery-row-6 { grid-template-columns: repeat(3, 1fr); }
    }
    @media (max-width: 640px) {
      header { padding: 48px 20px 56px; }
      section { padding: 28px 20px; margin-bottom: 20px; }
      h1 { font-size: 32px; }
      h2 { font-size: 20px; }
      .gallery-row { grid-template-columns: repeat(3, 1fr); gap: 10px; }
      .gallery-row-4 { grid-template-columns: repeat(2, 1fr); gap: 12px; }
      .gallery-row-6 { grid-template-columns: repeat(2, 1fr); gap: 10px; }
      figcaption { font-size: 12px; padding: 10px 12px; }
      .toc { gap: 8px; }
      .toc a { padding: 8px 14px; font-size: 12px; }
    }
  </style>
</head>
<body>
  <header>
    <div class="container">
      <h1>Project 5: Fun with Diffusion Models</h1>
      <div class="meta-grid">
        <div class="meta-card">CS180 · Fall 2025</div>
        <div class="meta-card">Author · Fangzhou</div>
      </div>
      <p class="lead">Diffusion forward and reverse processes, denoising strategies, image-to-image workflows, classifier-free guidance, and UNet training for flow matching.</p>
      <nav class="toc">
        <a href="#part0"><span>Part 0</span> Setup &amp; text prompts</a>
        <a href="#part1"><span>Part 1.1-1.2</span> Forward pass &amp; classical denoising</a>
        <a href="#part13"><span>Part 1.3</span> One-step denoising</a>
        <a href="#part14"><span>Part 1.4</span> Iterative denoising</a>
        <a href="#part15"><span>Part 1.5-1.6</span> Sampling &amp; CFG</a>
        <a href="#part17"><span>Part 1.7</span> Image-to-image &amp; inpainting</a>
        <a href="#part18"><span>Part 1.8-1.9</span> Visual anagrams &amp; hybrids</a>
        <a href="#partb1"><span>Part B.1</span> UNet &amp; single-step denoising</a>
        <a href="#partb2"><span>Part B.2</span> Flow matching model</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="container">
      <!-- Part 0 -->
      <section id="part0">
        <h2>Part 0: Setup and Playing with My Own Text Prompt</h2>
        <p>This part sets up the environment and plays with my own text prompt. We can observe that for prompts that are too abstract or OOD, the generated images are not very good. However, for prompts that are more specific and in-domain, the generated images are much better. The seed chosen is here is 100 for reproducibility.</p>
        <p>The first row shows the generated image with a inference step of 50, while the second row shows the generated image with a inference step of 100.</p>
        <div class="gallery">
          <figure>
            <img src="stage_2_output_inference_50_0.png" alt="a rocket ship, step 50" loading="lazy" />
            <figcaption>"a rocket ship", step 50</figcaption>
          </figure>
          <figure>
            <img src="stage_2_output_inference_50_1.png" alt="a photo of a dog, step 50" loading="lazy" />
            <figcaption>"a photo of a dog", step 50</figcaption>
          </figure>
          <figure>
            <img src="stage_2_output_inference_50_2.png" alt="a pencil, step 50" loading="lazy" />
            <figcaption>"a pencil", step 50</figcaption>
          </figure>
          <figure>
            <img src="stage_2_output_inference_100_0.png" alt="a rocket ship, step 100" loading="lazy" />
            <figcaption>"a rocket ship", step 100</figcaption>
          </figure>
          <figure>
            <img src="stage_2_output_inference_100_1.png" alt="a photo of a dog, step 100" loading="lazy" />
            <figcaption>"a photo of a dog", step 100</figcaption>
          </figure>
          <figure>
            <img src="stage_2_output_inference_100_2.png" alt="a pencil, step 100" loading="lazy" />
            <figcaption>"a pencil", step 100</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part 1.1 and 1.2 -->
      <section id="part1">
        <h2>Part 1.1: Implementing the Forward Process</h2>
        <p>The forward process adds noise to a clean image according to the diffusion noise schedule. Given a clean image \(x_0\), we can obtain the noisy image \(x_t\) at timestep \(t\) using the following formula:</p>
        <div class="formula">
          <p>\[ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) \]</p>
        </div>
        <p>Here, \(\bar{\alpha}_t\) is the cumulative product of the noise schedule, controlling how much of the original signal remains at each timestep. As \(t\) increases, more noise is added and the image becomes increasingly corrupted.</p>

        <h2>Part 1.2: Classical Denoising</h2>
        <p>A naive approach to denoising is to apply a Gaussian blur filter to the noisy images. While this removes some high-frequency noise, it also blurs the underlying image structure, resulting in poor reconstruction quality. The comparison below shows the noisy images at timesteps 250, 500, and 750, along with their Gaussian-filtered versions.</p>
        <div class="gallery gallery-wide">
          <figure class="wide">
            <img src="classical_denoised.png" alt="Forward Pass and Classical Denoising Comparison" loading="lazy" />
            <figcaption>Forward Pass Results (top) and Classical Gaussian Denoising (bottom) at t = 250, 500, 750</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part 1.3 -->
      <section id="part13">
        <h2>Part 1.3: One-Step Denoising</h2>
        <p>Instead of using Gaussian blur, we can use a pretrained UNet denoiser to estimate the noise and recover the clean image. Given the noisy image \(x_t\) and the predicted noise \(\hat{\epsilon}\), we can estimate the original image using:</p>
        <div class="formula">
          <p>\[ \hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}} \]</p>
        </div>
        <p>However, one-step denoising has limitations: the quality degrades significantly at higher noise levels because the UNet must make larger predictions. Below are the results at timesteps 250, 500, and 750.</p>
        <h3>t = 250:</h3>
        <div class="gallery gallery-small">
          <figure>
            <img src="one_step_denoise_250.png" alt="One-Step Denoising at Time Step 250" loading="lazy" />
            <figcaption>One-Step Denoising at Time Step 250</figcaption>
          </figure>
        </div>
        <h3>t = 500:</h3>
        <div class="gallery gallery-small">
          <figure>
            <img src="one_step_denoise_500.png" alt="One-Step Denoising at Time Step 500" loading="lazy" />
            <figcaption>One-Step Denoising at Time Step 500</figcaption>
          </figure>
        </div>
        <h3>t = 750:</h3>
        <div class="gallery gallery-small">
          <figure>
            <img src="one_step_denoise_750.png" alt="One-Step Denoising at Time Step 750" loading="lazy" />
            <figcaption>One-Step Denoising at Time Step 750</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part 1.4 -->
      <section id="part14">
        <h2>Part 1.4: Iterative Denoising</h2>
        <p>To achieve better results, we can iteratively denoise by taking smaller steps. Starting from a noisy image, we progressively remove noise using the following update rule:</p>
        <div class="formula">
          <p>\[ x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'}}\beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t'})}{1 - \bar{\alpha}_t} x_t + v_\sigma \]</p>
        </div>
        <p>where \(v_\sigma\) is random noise scaled appropriately. We use strided timesteps starting from 990 with stride 30, iteratively denoising until we reach timestep 0. This produces much cleaner results than one-step denoising.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="iterative_denoised_process.png" alt="Iterative Denoising Process" loading="lazy" />
            <figcaption>Iterative Denoising Process</figcaption>
          </figure>
        </div>
        <p>Here is a comparison between the classical denoising, the one-step denoising and the iterative denoising.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="iterative_denoised.png" alt="Comparison between Classical, One-Step, and Iterative Denoising" loading="lazy" />
            <figcaption>Comparison between the Classical Denoising, the One-Step Denoising and the Iterative Denoising</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part 1.5 and 1.6 -->
      <section id="part15">
        <h2>Part 1.5: Diffusion Model Sampling</h2>
        <p>We can generate images from scratch by starting from pure noise and iteratively denoising. Using the prompt "a high quality photo", we generate 5 sample images. The quality is reasonable but not optimal because we're using unconditional generation.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="samples.png" alt="Diffusion Model Sampling" loading="lazy" />
            <figcaption>Five samples generated from pure noise with prompt "a high quality photo"</figcaption>
          </figure>
        </div>

        <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
        <p>To improve generation quality, we use Classifier-Free Guidance which combines conditional and unconditional noise predictions:</p>
        <div class="formula">
          <p>\[ \epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u) \]</p>
        </div>
        <p>where \(\epsilon_u\) is the unconditional noise estimate, \(\epsilon_c\) is the conditional estimate, and \(\gamma\) is the guidance scale (we use \(\gamma = 7\)). Higher \(\gamma\) values produce images more aligned with the prompt but may reduce diversity.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="samples_iterative_denoise_cfg.png" alt="Classifier-Free Guidance" loading="lazy" />
            <figcaption>CFG sampling with \(\gamma = 7\) - significantly improved quality</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part 1.7 -->
      <section id="part17">
        <h2>Part 1.7.1: Image-to-Image Translation (SDEdit)</h2>
        <p>SDEdit allows us to edit images by adding noise and then denoising with a text prompt. The parameter <code>i_start</code> controls how much noise is added: lower values preserve more of the original image structure, while higher values allow more creative freedom. We apply this technique to the Campanile, a web image, and two hand-drawn sketches.</p>

        <h3>Campanile Image</h3>
        <div class="gallery-row">
          <figure><img src="campanile_edited_1.png" alt="Campanile i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="campanile_edited_3.png" alt="Campanile i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="campanile_edited_5.png" alt="Campanile i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="campanile_edited_7.png" alt="Campanile i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="campanile_edited_10.png" alt="Campanile i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="campanile_edited_20.png" alt="Campanile i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="campanile_edited_original.png" alt="Campanile original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h3>Tom and Jerry</h3>
        <div class="gallery-row">
          <figure><img src="tom_and_jerry_1.png" alt="Tom and Jerry i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="tom_and_jerry_3.png" alt="Tom and Jerry i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="tom_and_jerry_5.png" alt="Tom and Jerry i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="tom_and_jerry_7.png" alt="Tom and Jerry i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="tom_and_jerry_10.png" alt="Tom and Jerry i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="tom_and_jerry_20.png" alt="Tom and Jerry i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="tom_and_jerry.png" alt="Tom and Jerry original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h3>Hand-drawn House</h3>
        <div class="gallery-row">
          <figure><img src="drawn_cat_1.png" alt="Hand-drawn house i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="drawn_cat_3.png" alt="Hand-drawn house i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="drawn_cat_5.png" alt="Hand-drawn house i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="drawn_cat_7.png" alt="Hand-drawn house i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="drawn_cat_10.png" alt="Hand-drawn house i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="drawn_cat_20.png" alt="Hand-drawn house i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="drawn_cat.png" alt="Hand-drawn house original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h3>Hand-drawn Smiley Face</h3>
        <div class="gallery-row">
          <figure><img src="drawn_doraemon_1.png" alt="Hand-drawn smiley face i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="drawn_doraemon_3.png" alt="Hand-drawn smiley face i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="drawn_doraemon_5.png" alt="Hand-drawn smiley face i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="drawn_doraemon_7.png" alt="Hand-drawn smiley face i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="drawn_doraemon_10.png" alt="Hand-drawn smiley face i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="drawn_doraemon_20.png" alt="Hand-drawn smiley face i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="drawn_doraemon.png" alt="Hand-drawn smiley face original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h2>Part 1.7.2: Inpainting</h2>
        <p>Inpainting fills in masked regions of an image while keeping the rest unchanged. At each denoising step, we replace the unmasked region with a noisy version of the original:</p>
        <div class="formula">
          <p>\[ x_t \leftarrow \mathbf{m} \odot x_t + (1 - \mathbf{m}) \odot \text{forward}(x_{\text{orig}}, t) \]</p>
        </div>
        <p>where \(\mathbf{m}\) is the binary mask (1 = region to inpaint, 0 = keep original). This allows the diffusion model to generate new content only in the masked area while maintaining consistency with the surrounding context.</p>

        <h3>Campanile Inpainting</h3>
        <div class="gallery-row-4">
          <figure><img src="campanile_inpainted_Original.png" alt="Campanile original" loading="lazy" /><figcaption>Original</figcaption></figure>
          <figure><img src="campanile_mask_Mask.png" alt="Campanile mask" loading="lazy" /><figcaption>Mask</figcaption></figure>
          <figure><img src="campanile_mask_To Replace.png" alt="Campanile to replace" loading="lazy" /><figcaption>To Replace</figcaption></figure>
          <figure><img src="campanile_inpainted_Inpainted.png" alt="Campanile inpainted" loading="lazy" /><figcaption>Inpainted</figcaption></figure>
        </div>

        <h3>Dog Inpainting</h3>
        <div class="gallery-row-4">
          <figure><img src="dog_inpainted_Original.png" alt="Dog original" loading="lazy" /><figcaption>Original</figcaption></figure>
          <figure><img src="dog_mask_Mask.png" alt="Dog mask" loading="lazy" /><figcaption>Mask</figcaption></figure>
          <figure><img src="dog_mask_To Replace.png" alt="Dog to replace" loading="lazy" /><figcaption>To Replace</figcaption></figure>
          <figure><img src="dog_inpainted_Inpainted.png" alt="Dog inpainted" loading="lazy" /><figcaption>Inpainted</figcaption></figure>
        </div>

        <h3>Charger Inpainting</h3>
        <div class="gallery-row-4">
          <figure><img src="charger_inpainted_Original.png" alt="Charger original" loading="lazy" /><figcaption>Original</figcaption></figure>
          <figure><img src="charger_mask_Mask.png" alt="Charger mask" loading="lazy" /><figcaption>Mask</figcaption></figure>
          <figure><img src="charger_mask_To Replace.png" alt="Charger to replace" loading="lazy" /><figcaption>To Replace</figcaption></figure>
          <figure><img src="charger_inpainted_Inpainted.png" alt="Charger inpainted" loading="lazy" /><figcaption>Inpainted</figcaption></figure>
        </div>

        <h2>Part 1.7.3: Text-Conditional Image-to-image Translation</h2>
        <p>This extends SDEdit by using specific text prompts to guide the translation. Instead of the generic "a high quality photo" prompt, we provide descriptive prompts that transform the image toward a target concept while preserving the underlying structure. The process is the same as Part 1.7.1, but with targeted prompts that steer the generation in a specific semantic direction.</p>

        <h3>Campanile with Text Prompt</h3>
        <div class="gallery-row">
          <figure><img src="companile_transform_1.png" alt="Campanile transform i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="companile_transform_3.png" alt="Campanile transform i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="companile_transform_5.png" alt="Campanile transform i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="companile_transform_7.png" alt="Campanile transform i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="companile_transform_10.png" alt="Campanile transform i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="companile_transform_20.png" alt="Campanile transform i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="campanile_edited_original.png" alt="Campanile original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h3>Car with Text Prompt</h3>
        <div class="gallery-row">
          <figure><img src="car_edited_1.png" alt="Car i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="car_edited_3.png" alt="Car i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="car_edited_5.png" alt="Car i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="car_edited_7.png" alt="Car i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="car_edited_10.png" alt="Car i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="car_edited_20.png" alt="Car i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="car_edited_original.png" alt="Car original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>

        <h3>Kiwi with Text Prompt</h3>
        <div class="gallery-row">
          <figure><img src="kiwi_edited_1.png" alt="Kiwi i_start=1" loading="lazy" /><figcaption>i_start = 1</figcaption></figure>
          <figure><img src="kiwi_edited_3.png" alt="Kiwi i_start=3" loading="lazy" /><figcaption>i_start = 3</figcaption></figure>
          <figure><img src="kiwi_edited_5.png" alt="Kiwi i_start=5" loading="lazy" /><figcaption>i_start = 5</figcaption></figure>
          <figure><img src="kiwi_edited_7.png" alt="Kiwi i_start=7" loading="lazy" /><figcaption>i_start = 7</figcaption></figure>
          <figure><img src="kiwi_edited_10.png" alt="Kiwi i_start=10" loading="lazy" /><figcaption>i_start = 10</figcaption></figure>
          <figure><img src="kiwi_edited_20.png" alt="Kiwi i_start=20" loading="lazy" /><figcaption>i_start = 20</figcaption></figure>
          <figure><img src="kiwi_edited_original.png" alt="Kiwi original" loading="lazy" /><figcaption>original</figcaption></figure>
        </div>
      </section>

      <!-- Part 1.8 and 1.9 -->
      <section id="part18">
        <h2>Part 1.8: Visual Anagrams</h2>
        <p>Visual anagrams are images that look like different things when flipped upside down. To create these, we simultaneously denoise toward two different prompts - one for the upright view and one for the flipped view. At each denoising step:</p>
        <div class="formula">
          <p>\[ \epsilon_1 = \text{UNet}(x_t, t, p_1) \]</p>
          <p>\[ \epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2)) \]</p>
          <p>\[ \epsilon = \frac{\epsilon_1 + \epsilon_2}{2} \]</p>
        </div>
        <p>where \(p_1\) is the prompt for the upright image and \(p_2\) is the prompt for the flipped image. By averaging these noise estimates, the resulting image satisfies both prompts depending on its orientation.</p>
        <div class="gallery-row-6">
          <figure>
            <img src="flip_illusion_campfire_skull.png" alt="Flip illusion campfire skull" loading="lazy" />
            <figcaption>Campfire + Skull</figcaption>
          </figure>
          <figure>
            <img src="flip_illusion_campfire_skull.png" alt="Flip illusion campfire skull flipped" style="transform: rotate(180deg);" loading="lazy" />
            <figcaption>Campfire + Skull (flipped)</figcaption>
          </figure>
          <figure>
            <img src="flip_illusion_old_man_campfire.png" alt="Flip illusion old man campfire" loading="lazy" />
            <figcaption>Old Man + Campfire</figcaption>
          </figure>
          <figure>
            <img src="flip_illusion_old_man_campfire.png" alt="Flip illusion old man campfire flipped" style="transform: rotate(180deg);" loading="lazy" />
            <figcaption>Old Man + Campfire (flipped)</figcaption>
          </figure>
          <figure>
            <img src="flip_illusion_old_man_village.png" alt="Flip illusion old man village" loading="lazy" />
            <figcaption>Old Man + Village</figcaption>
          </figure>
          <figure>
            <img src="flip_illusion_old_man_village.png" alt="Flip illusion old man village flipped" style="transform: rotate(180deg);" loading="lazy" />
            <figcaption>Old Man + Village (flipped)</figcaption>
          </figure>
        </div>

        <h2>Part 1.9: Hybrid Images</h2>
        <p>Hybrid images appear as different things when viewed from different distances. This exploits the fact that humans perceive low frequencies at a distance and high frequencies up close. We create these by combining the low-frequency component of one noise estimate with the high-frequency component of another:</p>
        <div class="formula">
          <p>\[ \epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2) \]</p>
        </div>
        <p>where \(\epsilon_1\) is the noise estimate for the low-frequency (distant view) prompt and \(\epsilon_2\) is for the high-frequency (close-up view) prompt. The lowpass and highpass filters are complementary Gaussian filters.</p>
        <div class="gallery">
          <figure>
            <img src="hybrid_campfire_skull.png" alt="Hybrid campfire skull" loading="lazy" />
            <figcaption>Hybrid Images, Campfire + Skull</figcaption>
          </figure>
          <figure>
            <img src="hybrid_rocket_ship_pencil.png" alt="Hybrid rocket ship pencil" loading="lazy" />
            <figcaption>Hybrid Images, Rocket Ship + Pencil</figcaption>
          </figure>
          <figure>
            <img src="hybrid_skull_waterfalls.png" alt="Hybrid skull waterfalls" loading="lazy" />
            <figcaption>Hybrid Images, Skull + Waterfalls</figcaption>
          </figure>
        </div>
      </section>

      <!-- Part B.1 -->
      <section id="partb1">
        <h2>Part B.1.1 and Part B.1.2.0: Implement the UNet and Visualizing the Noisy Images</h2>
        <p>We implement a UNet architecture for denoising MNIST digits. The noising process adds Gaussian noise to clean images:</p>
        <div class="formula">
          <p>\[ z = x + \sigma \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) \]</p>
        </div>
        <p>Here \(\sigma\) is the noise level (standard deviation). The following images show examples at different noise levels from 0.0 to 1.0.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="noisy_images.png" alt="Noisy images visualization" loading="lazy" />
            <figcaption>Visualization of the Noisy Images</figcaption>
          </figure>
        </div>

        <h2>Part B.1.2.1: Training</h2>
        <p>The UNet is trained on the MNIST dataset with a fixed noise level \(\sigma = 0.5\). We optimize the L2 loss between the denoised output and the clean image:</p>
        <div class="formula">
          <p>\[ \mathcal{L} = \mathbb{E}_{z,x} \| D_\theta(z) - x \|^2 \]</p>
        </div>
        <p>where \(D_\theta\) is our denoiser network. Below are the training loss curve and the denoised results after the first and final epochs.</p>
        <div class="gallery gallery-two">
          <figure>
            <img src="ood_testing.png" alt="Denoising comparison" loading="lazy" />
            <figcaption>Comparison between the denoised image after the first and the final epoch</figcaption>
          </figure>
          <figure>
            <img src="training_curve.png" alt="Training loss curve" loading="lazy" />
            <figcaption>Training Loss Curve</figcaption>
          </figure>
        </div>

        <h2>Part B.1.2.2: Out-of-Distribution Testing</h2>
        <p>This section tests the trained UNet's performance on out-of-distribution data, ie. images with different noise levels. With a larger noise level, the denoising performance seems to drop, which makes sense since the model is not trained on such levels.</p>
        <div class="gallery gallery-small">
          <figure>
            <img src="denoising_grid.png" alt="OOD testing" loading="lazy" />
            <figcaption>Performance of the denoiser on OOD noise levels</figcaption>
          </figure>
        </div>

        <h2>Part B.1.2.3: Denoising Pure Noise</h2>
        <p>This section trains a UNet to denoise a pure noise. Since the training is not conditioned on the classes, we expect the model to output images that don't look like actual numbers.</p>
        <div class="gallery gallery-two">
          <figure>
            <img src="pure_noise_denoising.png" alt="Pure noise denoising" loading="lazy" />
            <figcaption>Sampled Results on Pure Noise for the Model after the First and the Final epoch</figcaption>
          </figure>
          <figure>
            <img src="training_curve_pure_noise.png" alt="Training curve pure noise" loading="lazy" />
            <figcaption>Training Loss Curve for the UNet Trained for Denoising Pure Noises</figcaption>
          </figure>
        </div>
        <p>Notice that the outputted images have features of different numbers in the training dataset. This is because the training is not conditioned on classes; therefore, pure noise can be denoised to any number during the training process.</p>
      </section>

      <!-- Part B.2 -->
      <section id="partb2">
        <h2>Part B.2: Training a Flow Matching Model</h2>
        <p>The one-step denoiser doesn't work well for pure noise. Therefore, we implement a flow matching model that iteratively transforms noise into clean images. The key idea is to learn a velocity field that describes how to move from noise to data.</p>
        <p>We define an interpolation between noise \(x_0 \sim \mathcal{N}(0, I)\) and clean data \(x_1\):</p>
        <div class="formula">
          <p>\[ x_t = (1 - t) x_0 + t \cdot x_1, \quad t \in [0, 1] \]</p>
        </div>
        <p>The velocity (or flow) at any point is simply:</p>
        <div class="formula">
          <p>\[ v_t = \frac{d}{dt} x_t = x_1 - x_0 \]</p>
        </div>
        <p>We train the network \(u_\theta\) to predict this velocity using the flow matching loss:</p>
        <div class="formula">
          <p>\[ \mathcal{L}_{\text{FM}} = \mathbb{E}_{t, x_0, x_1} \| u_\theta(x_t, t) - (x_1 - x_0) \|^2 \]</p>
        </div>
        <p>During sampling, we start from pure noise \(x_0\) and iteratively apply the predicted velocity to reach the clean image.</p>

        <h2>Part B.2.1 and Part B.2.2: Adding Time Conditioning to UNet and Training the UNet</h2>
        <p>To enable the UNet to handle different timesteps, we add time conditioning. The scalar \(t \in [0, 1]\) is embedded via fully-connected blocks (FCBlocks) and used to modulate the intermediate feature maps. This allows the network to learn different behaviors at different points along the flow trajectory.</p>
        <div class="gallery gallery-two">
          <figure>
            <img src="time_conditioned_sampling.png" alt="Time-conditioned sampling" loading="lazy" />
            <figcaption>Sampled Results from the Time-Conditioned UNet</figcaption>
          </figure>
          <figure>
            <img src="training_curve_time_conditioned.png" alt="Time-conditioned training" loading="lazy" />
            <figcaption>Training Loss Curve for the Time-Conditioned UNet</figcaption>
          </figure>
        </div>

        <h2>Part B.2.3 and Part B.2.4: Adding Class-Conditioning to UNet and Training the UNet</h2>
        <p>To generate specific digit classes, we add class conditioning using one-hot encoded class vectors \(c\). Similar to time conditioning, the class embedding is processed through FCBlocks and combined with the features. The network now takes both time and class as input: \(u_\theta(x_t, c, t)\). This enables controllable generation of specific digits (0-9).</p>
        <div class="gallery gallery-two">
          <figure>
            <img src="class_samples_epochFinal.png" alt="Class samples final" loading="lazy" />
            <figcaption>Sampled Results of the Class-Conditioned Unet after the Final Epoch</figcaption>
          </figure>
          <figure>
            <img src="training_curve_class_conditioned.png" alt="Class-conditioned training" loading="lazy" />
            <figcaption>Training Loss Curve for the Class-Conditioned UNet</figcaption>
          </figure>
        </div>

        <h2>Part B.2.5: Sampling from the UNet</h2>
        <p>To generate images, we start with pure noise \(x_0 \sim \mathcal{N}(0, I)\) and iteratively apply the predicted velocity over \(T\) steps:</p>
        <div class="formula">
          <p>\[ x_{t+\Delta t} = x_t + \Delta t \cdot u_\theta(x_t, c, t) \]</p>
        </div>
        <p>The following images demonstrate the generation quality after the first, fifth, and final epochs. More training leads to better digit generation.</p>
        <div class="gallery">
          <figure>
            <img src="class_samples_epoch0.png" alt="Class samples epoch 0" loading="lazy" />
            <figcaption>Sampled Results of the Class-Conditioned Unet after the First Epoch</figcaption>
          </figure>
          <figure>
            <img src="class_samples_epochHalf.png" alt="Class samples mid" loading="lazy" />
            <figcaption>Sampled Results of the Class-Conditioned Unet after the Fifth Epoch</figcaption>
          </figure>
          <figure>
            <img src="class_samples_epochFinal.png" alt="Class samples final" loading="lazy" />
            <figcaption>Sampled Results of the Class-Conditioned Unet after the Final Epoch</figcaption>
          </figure>
        </div>

        <p>To keep the training process simple, we can get rid of the scheduler and use a constant learning rate instead (perhaps with a smaller learning rate and more training steps). I choose to train the model with a constant learning rate of 1e-3 for 20 epochs. The following images demonstrate the training and sampled results.</p>
        <div class="gallery gallery-two">
          <figure>
            <img src="class_samples_epochFinal_no_scheduler.png" alt="Class samples final no scheduler" loading="lazy" />
            <figcaption>Sampled Results of the Class-Conditioned Unet after the Final Epoch without the Scheduler</figcaption>
          </figure>
          <figure>
            <img src="training_curve_class_conditioned_no_scheduler.png" alt="Class-conditioned no scheduler" loading="lazy" />
            <figcaption>Training Loss Curve for the Class-Conditioned UNet without the Scheduler</figcaption>
          </figure>
        </div>
      </section>
    </div>
  </main>

  <footer>
    <div class="container">© Fangzhou — CS180 Project 5 (for course submission/testing)</div>
  </footer>
</body>
</html>
